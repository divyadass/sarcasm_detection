{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_Project_Sarcasm_Detection_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyadass/sarcasm_detection/blob/develop/NLP_Project_Sarcasm_Detection_Questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-YKobEcth0q"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "1. This jupyter notebook will provide a basic walkthrough for text classificaiton using keras on text data.\n",
        "\n",
        "2. To see the deployment of the model trained here, please switch to deployment branch of the repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp68FAQf9aMN"
      },
      "source": [
        "# Sarcasm Detection\n",
        " **Acknowledgement**\n",
        "\n",
        "Misra, Rishabh, and Prahal Arora. \"Sarcasm Detection using Hybrid Neural Network.\" arXiv preprint arXiv:1908.07414 (2019)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3Wj_mIZ8S3K"
      },
      "source": [
        "## Install `Tensorflow2.0` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW2Uk8otQvi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb62a6c6-5ffa-4657-9617-436bc9aa397c"
      },
      "source": [
        "!!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "  Downloading tensorflow-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (86.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n",
            "  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n",
            "\u001b[K     |████████████████████████████████| 449 kB 48.5 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.40.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (0.37.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==2.0.0) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=1526ca29014e3e064a9acc1bd030782ed1e8a1b5b3a2411dc79bc388ac86d0f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9kv9tyJ77eF"
      },
      "source": [
        "## Get Required Files from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0O_n6OIEVyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a37444f-3f17-43c7-b49a-19113547cfa5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMHXboncuQM4"
      },
      "source": [
        "project_path = '/content/'\n",
        "base_path = 'drive/My Drive/Sarcasm Detection/Data/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2AbRvIGudTI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e91b57c0-09c5-4476-bf24-5d8defc42884"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_D7YRWptxjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0ac464f-9110-4ce2-dd58-2bcf160a1759"
      },
      "source": [
        "ls 'drive/My Drive/Sarcasm Detection/Data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove.6B.100d.txt  glove.6B.zip  Sarcasm_Headlines_Dataset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXYwajPeQbRq"
      },
      "source": [
        "#**## Reading and Exploring Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAk6BRUh8CqL"
      },
      "source": [
        "## Read Data \"Sarcasm_Headlines_Dataset.json\" and basic exploration to get some insights about the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StSLB-T8PuGr"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7--SMwXjvBSR"
      },
      "source": [
        "data_s = pd.read_json(base_path+'Sarcasm_Headlines_Dataset.json', lines= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11EZr_OSyXkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa168dba-da66-4fec-eef5-575c99389aaa"
      },
      "source": [
        "data_s.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26709, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlCoDVWVveS4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b7e753c0-3fff-4e3a-8e27-fd71523ae2c7"
      },
      "source": [
        "data_s.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        article_link  ... is_sarcastic\n",
              "0  https://www.huffingtonpost.com/entry/versace-b...  ...            0\n",
              "1  https://www.huffingtonpost.com/entry/roseanne-...  ...            0\n",
              "2  https://local.theonion.com/mom-starting-to-fea...  ...            1\n",
              "3  https://politics.theonion.com/boehner-just-wan...  ...            1\n",
              "4  https://www.huffingtonpost.com/entry/jk-rowlin...  ...            0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UXgbHJNyRUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69fe2503-4ffb-442e-f1a8-aec112b862ac"
      },
      "source": [
        "## Dropping duplicates rows\n",
        "data_s.drop_duplicates(inplace=True)\n",
        "print('dataframe shape: ', data_s.shape)\n",
        "\n",
        "## Removing rows with duplicates headlines since keeping duplicates in headline gives model no useful infoemation\n",
        "data_s.drop_duplicates(subset='headline', keep='last', inplace=True)\n",
        "print('dataframe shape: ', data_s.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataframe shape:  (26708, 3)\n",
            "dataframe shape:  (26602, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYt92V-GzVNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d98f8f-377f-47bb-909e-1f564aefc3fb"
      },
      "source": [
        "print('Dataset size after cleaning: ',data_s.shape)\n",
        "print('Number of unique articles link: '+str(data_s['article_link'].unique().shape[0]))\n",
        "print('Number of unique headline: '+ str(data_s['headline'].unique().shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size after cleaning:  (26602, 3)\n",
            "Number of unique articles link: 26602\n",
            "Number of unique headline: 26602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPc-ax49zwGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198f3b6d-6e0b-499f-a9ad-618f1452a2e0"
      },
      "source": [
        "print('Sarcastic Comments: ', data_s['is_sarcastic'].value_counts()[1])\n",
        "print('Non-Sarcastic Comments: ', data_s['is_sarcastic'].value_counts()[0])\n",
        "print()\n",
        "print('Sarcastic Comments percentage: ', data_s['is_sarcastic'].value_counts()[1]/data_s.shape[0]*100)\n",
        "print('Non-Sarcastic Comments percentage: ', data_s['is_sarcastic'].value_counts()[0]/data_s.shape[0]*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sarcastic Comments:  11651\n",
            "Non-Sarcastic Comments:  14951\n",
            "\n",
            "Sarcastic Comments percentage:  43.79745883768138\n",
            "Non-Sarcastic Comments percentage:  56.20254116231862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6pXf7A78E2H"
      },
      "source": [
        "## Dropping `article_link` from dataset\n",
        "As we only need headline text data and is_sarcastic column for this project. We can drop artical link column here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLSVsvrlP9qD"
      },
      "source": [
        "data_s.drop(['article_link'], axis= 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPvIv-8H0lxh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "c352f940-be00-47f0-caaa-93e16ef640cd"
      },
      "source": [
        "data_s.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline  is_sarcastic\n",
              "0  former versace store clerk sues over secret 'b...             0\n",
              "1  the 'roseanne' revival catches up to our thorn...             0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0h6IOxU8OdH"
      },
      "source": [
        "\n",
        "\n",
        "## Get the Length of each line and find the maximum length\n",
        "As different lines are of different length. We need to pad the our sequences using the max length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRAsChZAQmr3"
      },
      "source": [
        "data_s['headline_length'] = data_s['headline'].str.split().str.len()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koq6_tlw2lqG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "536a6d18-4b5a-4a6b-ba01-7aef0c006eaf"
      },
      "source": [
        "data_s.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline  ...  headline_length\n",
              "0  former versace store clerk sues over secret 'b...  ...               12\n",
              "1  the 'roseanne' revival catches up to our thorn...  ...               14\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcJCuzox79Wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34118a2-31f7-419d-e84c-f68f67ae1667"
      },
      "source": [
        "data_s['headline_length'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    26602.000000\n",
              "mean         9.856214\n",
              "std          3.165456\n",
              "min          2.000000\n",
              "25%          8.000000\n",
              "50%         10.000000\n",
              "75%         12.000000\n",
              "max         39.000000\n",
              "Name: headline_length, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_TSLsIz2oMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7f0b0b-2fbe-4432-88ee-d75231b537f2"
      },
      "source": [
        "print('Maximum length for a headline: ',data_s['headline_length'].max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length for a headline:  39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPPd0YuPXi2M"
      },
      "source": [
        "#**## Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35abKfRx8as3"
      },
      "source": [
        "## Import required modules required for modelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cr7m2dGaX4W"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Flatten, Bidirectional\n",
        "from tensorflow.keras.models import Model, Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ziybaD1RdD9"
      },
      "source": [
        "# Setting Different Parameters for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmu9nGkPhctY"
      },
      "source": [
        "max_features = 10000  ## no of unique words in the vocabulary\n",
        "maxlen = 15 ## no of words to use from each headline\n",
        "embedding_size = 100 ## length of word embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abSe-bM8fn9"
      },
      "source": [
        "## Applying Keras Tokenizer to headline column of the data.\n",
        "- Create a tokenizer instance using Tokenizer(num_words=max_features) \n",
        "- Fitting this tokenizer instance on our data column df['headline'] using fit_on_texts()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyVugoAM9UnK"
      },
      "source": [
        "tokenizer = Tokenizer(\n",
        "    num_words=max_features+1,\n",
        "     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "     lower=True,\n",
        "     split=\" \",\n",
        "     oov_token = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ-C9NaB-gVZ"
      },
      "source": [
        "tokenizer.fit_on_texts(data_s['headline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqF9p2ed-hXt"
      },
      "source": [
        "data_s['tokenized_headline'] = tokenizer.texts_to_sequences(data_s['headline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9Ad26HfTFMS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3d80b8db-6c73-4e85-ac33-7b66b09ee5bd"
      },
      "source": [
        "data_s.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "      <th>headline_length</th>\n",
              "      <th>tokenized_headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>[308, 1, 677, 3611, 2293, 48, 382, 2566, 1, 6,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>[4, 8414, 3332, 2741, 22, 2, 165, 8415, 414, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>[145, 837, 2, 903, 1740, 2086, 581, 4712, 220,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>[1478, 36, 222, 403, 2, 1823, 29, 319, 22, 10,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>[764, 717, 4713, 904, 1, 621, 592, 5, 4, 95, 1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline  ...                                 tokenized_headline\n",
              "0  former versace store clerk sues over secret 'b...  ...  [308, 1, 677, 3611, 2293, 48, 382, 2566, 1, 6,...\n",
              "1  the 'roseanne' revival catches up to our thorn...  ...  [4, 8414, 3332, 2741, 22, 2, 165, 8415, 414, 3...\n",
              "2  mom starting to fear son's web series closest ...  ...  [145, 837, 2, 903, 1740, 2086, 581, 4712, 220,...\n",
              "3  boehner just wants wife to listen, not come up...  ...  [1478, 36, 222, 403, 2, 1823, 29, 319, 22, 10,...\n",
              "4  j.k. rowling wishes snape happy birthday in th...  ...  [764, 717, 4713, 904, 1, 621, 592, 5, 4, 95, 1...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ffi63KsST3P"
      },
      "source": [
        "# Define X and y for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnjxBdqmSS4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660f2568-3df3-4bb9-9cc4-011fa3cd7df3"
      },
      "source": [
        "X = data_s['tokenized_headline']\n",
        "X = pad_sequences(X, maxlen = maxlen, value=0.0)\n",
        "y = np.asarray(data_s['is_sarcastic'])\n",
        "\n",
        "print(\"Number of Samples:\", len(X))\n",
        "print(X[0])\n",
        "print(\"Number of Labels: \", len(y))\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Samples: 26602\n",
            "[   0    0    0  308    1  677 3611 2293   48  382 2566    1    6 2567\n",
            " 8413]\n",
            "Number of Labels:  26602\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJLyKg-98rH_"
      },
      "source": [
        "## the Vocabulary size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-2w0gHEUUIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b495e51b-e826-4e35-c2aa-b4edbb8a042a"
      },
      "source": [
        "print('Number of words originally present:', len(tokenizer.word_index) + 1)\n",
        "\n",
        "print('Numer of words in our vocabulary: ', max_features, '  .Since, we had set num_words argumnet while defining tokenizer object')\n",
        "num_words = max_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words originally present: 29658\n",
            "Numer of words in our vocabulary:  10000   .Since, we had set num_words argumnet while defining tokenizer object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hjeMi40XcB1"
      },
      "source": [
        "#**## Word Embedding**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUF1TuQa8ux0"
      },
      "source": [
        "## Get Glove Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdFrigBvFNtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16af8cd2-1acb-4125-a592-1fc8607df9fa"
      },
      "source": [
        "ls 'drive/My Drive/Sarcasm Detection/Data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove.6B.100d.txt  glove.6B.zip  Sarcasm_Headlines_Dataset.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq5AIfRtMeZh"
      },
      "source": [
        "glove_file = base_path + \"glove.6B.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJLX_n2WMecA"
      },
      "source": [
        "#Extract Glove embedding zip file    ###### needed only once\n",
        "\n",
        "from zipfile import ZipFile\n",
        "with ZipFile(glove_file, 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IuXlu8-U3HG"
      },
      "source": [
        "# Getting the Word Embeddings using Embedding file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elZ-T5aFGZmZ"
      },
      "source": [
        "EMBEDDING_FILE = base_path+'/glove.6B.100d.txt'\n",
        "\n",
        "embeddings = {}\n",
        "for o in open(EMBEDDING_FILE):\n",
        "    word = o.split(\" \")[0]\n",
        "    # print(word)\n",
        "    embd = o.split(\" \")[1:]\n",
        "    embd = np.asarray(embd, dtype='float32')\n",
        "    # print(embd)\n",
        "    embeddings[word] = embd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nrlg-MdsfIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0959cc40-5e8b-4074-9b8e-4955413e36af"
      },
      "source": [
        "len(embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Ja0qcaGX8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f526aba-8575-479b-a613-a4b52fe8fd86"
      },
      "source": [
        "## viewing embedding for the words 'attendtion'\n",
        "embeddings['attention']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-3.3414e-01,  4.6667e-01,  5.3744e-01,  5.7743e-02,  2.9642e-01,\n",
              "        2.5224e-01, -6.5586e-01, -4.1668e-01,  2.1959e-01, -4.9413e-01,\n",
              "       -2.1816e-01, -9.0227e-02, -3.5179e-02, -2.7279e-01, -1.2343e-01,\n",
              "        1.6808e-01, -5.0623e-01, -4.0497e-01, -1.6763e-01,  4.9066e-01,\n",
              "       -8.8020e-02, -1.2339e-01, -3.8436e-01, -2.7766e-01, -1.3403e-01,\n",
              "        1.4342e-01, -2.9177e-01, -2.1146e-02,  5.2180e-01, -2.1213e-01,\n",
              "        3.0860e-02,  1.0402e-01, -1.6807e-01,  4.6170e-01, -5.4806e-01,\n",
              "       -6.6849e-02, -3.3180e-01,  3.7257e-01, -7.4962e-01,  6.2741e-01,\n",
              "       -4.9500e-01, -4.0996e-01, -1.4686e-01, -2.7166e-01, -7.7093e-02,\n",
              "       -2.8342e-01,  6.3663e-02, -1.5734e-01,  6.9649e-01, -9.6694e-01,\n",
              "        4.4510e-01, -2.4521e-01, -4.8447e-01,  1.1957e+00,  2.9929e-02,\n",
              "       -2.0425e+00, -2.8603e-01, -3.9043e-01,  1.2197e+00, -4.7760e-01,\n",
              "       -2.1191e-02,  9.3080e-01, -1.8173e-01, -7.5721e-02,  1.1242e+00,\n",
              "       -8.2276e-02,  5.7149e-02, -2.3585e-01,  3.5901e-01,  6.9223e-01,\n",
              "        3.0860e-02, -2.6091e-01, -3.0284e-01, -3.9094e-01,  2.1887e-01,\n",
              "        3.7618e-01, -1.5990e-01, -2.7495e-01, -8.3322e-01, -1.7576e-01,\n",
              "        5.7404e-01, -3.6228e-01,  2.2346e-01, -4.6975e-01, -1.4354e+00,\n",
              "       -2.4484e-01, -6.6958e-01, -5.4293e-03, -5.7340e-01, -6.3122e-01,\n",
              "        9.8732e-03, -1.9317e-01,  3.0410e-01, -2.6329e-01, -1.0366e-03,\n",
              "        3.6515e-01, -9.3829e-02, -6.7265e-02,  5.9425e-01,  6.7652e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTPxveDmVCrA"
      },
      "source": [
        "# Creating a weight matrix for words in training docs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-SwmlE2JqKq"
      },
      "source": [
        "from itertools import islice\n",
        "\n",
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a dict\"\n",
        "    return dict(islice(iterable, n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPE7sxEDttH7"
      },
      "source": [
        "## get all the words in our vocabulary \n",
        "# tokenizer.word_index.items()   ## commented since the count is 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF8KVCvlv4Lw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f4ac1ff-7c3c-4d83-f890-f4ed2c4d0481"
      },
      "source": [
        "num_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-VqDdRKJamN"
      },
      "source": [
        "vocab = take(num_words, tokenizer.word_index.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXHcMKWytE47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58539b1-8d88-4e43-e47d-05550af1782a"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQgOhiywU9nU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2e22ab-5767-47e2-ebbc-cf11babcbe14"
      },
      "source": [
        "embedding_matrix = np.zeros((num_words+1, 100))\n",
        "\n",
        "for word, i in vocab.items():\n",
        "    embedding_vector = embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "len(embeddings.values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL_ic7bkIOFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e114127d-66a1-4a1c-c3ec-fb535269e118"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10001, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7IbWuEX82Ra"
      },
      "source": [
        "## Create and Compile the Model\n",
        "Using Sequential model instance and then adding Embedding layer, Bidirectional(LSTM) layer, then dense and dropout layers as required. \n",
        "In the end adding a final dense layer with sigmoid activation for binary classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7jhsSgYXG4l"
      },
      "source": [
        "### Embedding layer for hint \n",
        "## model.add(Embedding(num_words, embedding_size, weights = [embedding_matrix]))\n",
        "### Bidirectional LSTM layer for hint \n",
        "## model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
        "\n",
        "# Define the Keras model\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words + 1, embedding_size, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
        "model.add(Dropout(0.50))\n",
        "\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.50))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.50))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYktDUP4m8q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0990e722-aec8-4976-8ad6-4ddb21d39814"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 15, 100)           1000100   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 15, 100)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 15, 256)           234496    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 15, 256)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3840)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3840)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 3841      \n",
            "=================================================================\n",
            "Total params: 1,238,437\n",
            "Trainable params: 238,337\n",
            "Non-trainable params: 1,000,100\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJFMxZwMWoTw"
      },
      "source": [
        "# Fit the model with a batch size of 100 and validation_split = 0.2. and state the validation accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpVkajCcWnRK"
      },
      "source": [
        "##Model Configuration\n",
        "batch_size = 100\n",
        "number_of_epochs = 5\n",
        "\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer = 'adam'\n",
        "additional_metrics = ['accuracy']\n",
        "verbosity_mode = True\n",
        "validation_split = 0.20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JURR5ooQoJx-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5694a8a1-73dd-4a79-a6de-479e0721b8ef"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y, epochs=number_of_epochs,\n",
        "                    batch_size=batch_size, \n",
        "                    verbose=verbosity_mode, \n",
        "                    validation_split=validation_split)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "213/213 [==============================] - 13s 20ms/step - loss: 0.6127 - accuracy: 0.6588 - val_loss: 0.4880 - val_accuracy: 0.7760\n",
            "Epoch 2/5\n",
            "213/213 [==============================] - 3s 16ms/step - loss: 0.5258 - accuracy: 0.7416 - val_loss: 0.4235 - val_accuracy: 0.8113\n",
            "Epoch 3/5\n",
            "213/213 [==============================] - 3s 16ms/step - loss: 0.4667 - accuracy: 0.7787 - val_loss: 0.3843 - val_accuracy: 0.8277\n",
            "Epoch 4/5\n",
            "213/213 [==============================] - 3s 16ms/step - loss: 0.4364 - accuracy: 0.7980 - val_loss: 0.3664 - val_accuracy: 0.8371\n",
            "Epoch 5/5\n",
            "213/213 [==============================] - 3s 16ms/step - loss: 0.4141 - accuracy: 0.8084 - val_loss: 0.3532 - val_accuracy: 0.8465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtOwdCNJrz7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5131e0a5-c5bd-4c5a-be27-1721a02be819"
      },
      "source": [
        "print('Validaiton accuracy at the end of 5th epoch is: ', history.history['val_accuracy'][-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validaiton accuracy at the end of 5th epoch is:  0.8464574217796326\n"
          ]
        }
      ]
    }
  ]
}